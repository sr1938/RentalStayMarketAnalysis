{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.4 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 2880 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 5\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 5\n",
						"Session ID: 670cd4e4-b90a-4491-9851-9401cab605f9\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.4\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session 670cd4e4-b90a-4491-9851-9401cab605f9 to get into ready status...\n",
						"Session 670cd4e4-b90a-4491-9851-9401cab605f9 has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from awsglue.transforms import ApplyMapping, DropNullFields\n",
				"from pyspark.sql.functions import lit, rand, col, regexp_replace, when, round\n",
				"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, LongType\n",
				"from pyspark.sql.functions import col, floor\n",
				"from pyspark.sql.functions import col, mean, round,lit\n",
				"\n",
				"\n",
				"\n",
				"\n",
				"# rds data with col cancellation policy\n",
				"\n",
				"df_rds_new = spark.read.format('parquet') \\\n",
				"    .option(\"mode\", \"PERMISSIVE\") \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .load('s3://airbnbrawzone/rdsdata/rdsdata.parquet/part-00000-ee488faa-43e4-4992-b03a-cd38d58a1bfd-c000.snappy.parquet')\n",
				"\n",
				"# old data\n",
				"df = spark.read.format('csv').options(sep=\",\", escape='\"', mode=\"PERMISSIVE\", header=True, multiLine=True).load('s3://final-044/zip/total_data.csv')\n",
				"\n",
				"\n",
				"selected_columns = [\n",
				"\"id\", \"name\", \"host_name\",\"last_scraped\", \"host_response_time\", \"host_listings_count\", \"host_verifications\",\"host_since\",\"host_is_superhost\",\n",
				"\"host_identity_verified\", \"neighbourhood\", \"city\", \"latitude\", \"longitude\", \"property_type\",\n",
				"\"room_type\", \"accommodates\", \"bathrooms\", \"bedrooms\", \"beds\", \"amenities\", \"price\",\n",
				"\"security_deposit\", \"guests_included\", \"extra_people\", \"availability_30\", \"availability_60\",\n",
				"\"availability_90\", \"availability_365\", \"number_of_reviews\", \"review_scores_rating\",\n",
				"\"instant_bookable\", \"month\", \"minimum_minimum_nights\", \"maximum_maximum_nights\",\n",
				"\"calculated_host_listings_count\",\"cancellation_policy\",\"last_review\"\n",
				"]\n",
				"\n",
				"# Selecting columns\n",
				"df = df.select(selected_columns)\n",
				"df1_rds_new = df_rds_new.select(selected_columns)\n",
				"\n",
				"\n",
				"# Union dataframes\n",
				"new_df = df.union(df1_rds_new)\n",
				"\n",
				"\n",
				"\n",
				"#handle duplicates\n",
				"df8=new_df.dropDuplicates()\n",
				"new_df=df8\n",
				"\n",
				"\n",
				"\n",
				"\n",
				"# replace $\n",
				"columns_to_replace = [\"price\", \"security_deposit\", \"extra_people\"]\n",
				"for col_name in columns_to_replace:\n",
				"    new_df = new_df.withColumn(col_name, regexp_replace(col(col_name), \"\\\\$\", \"\"))\n",
				"\n",
				"# 2)name:\n",
				"new_df = new_df.dropna(subset=[\"name\"])\n",
				"\n",
				"# 3)host_name:\n",
				"new_df = new_df.dropna(subset=[\"host_name\"])\n",
				"\n",
				"\n",
				"#4)\"last_scraped\":\n",
				"#new_df.filter(col(\"last_scraped\").isNull()).count()\n",
				"#---> 0\n",
				"\n",
				"\n",
				"\n",
				"#5)host_response_time:\n",
				"\n",
				"#mode_time = new_df.groupBy(\"host_response_time\").count().orderBy(col(\"count\").desc()).limit(1).select(\"host_response_time\").collect()[0][0]\n",
				"mode_time= \"within an hour\"\n",
				"new_df = new_df.fillna({\"host_response_time\": \"within an hour\"})\n",
				"\n",
				"\n",
				"\n",
				"#6) host_listings_count:\n",
				"new_df = new_df.withColumn(\"host_listings_count\", round(col(\"host_listings_count\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"#7)\"host_is_superhost\":\n",
				"#new_df.filter(col(\"host_is_superhost\").isNull()).count()\n",
				"#---> 7297\n",
				"new_df = new_df.dropna(subset=[\"host_is_superhost\"])\n",
				"\n",
				"#8)\"host_since\":\n",
				"#new_df.filter(col(\"host_since\").isNull()).count()\n",
				"#---> 403\n",
				"new_df = new_df.dropna(subset=[\"host_since\"])\n",
				"\n",
				"\n",
				"#9)\"last_review\":\n",
				"#new_df.filter(col(\"last_review\").isNull()).count()\n",
				"#---> 397671\n",
				"new_df = new_df.fillna(\"NA\")\n",
				"\n",
				"\n",
				"#11)neighbourhood:\n",
				"\n",
				"# Replace null values in the neighbourhood column with \"Unknown\"\n",
				"new_df = new_df.withColumn(\"neighbourhood\", when(new_df[\"neighbourhood\"].isNull(), \"Unknown\").otherwise(new_df[\"neighbourhood\"]))\n",
				"\n",
				"\n",
				"#12)city:\n",
				"\n",
				"default_value = \"Rio\"\n",
				"new_df = new_df.withColumn(\"city\", when(new_df[\"city\"].isNull(), default_value).otherwise(new_df[\"city\"]))\n",
				"\n",
				"\n",
				"#15)bathrooms:\n",
				"# replace null value using mean value\n",
				"\n",
				"from pyspark.sql.functions import col, mean, round,lit\n",
				"#mean_bathrooms = new_df.select(mean(col('bathrooms'))).collect()[0][0]\n",
				"#--> 1.6951  ====> 1.0\n",
				"rounded_mean_bathrooms = 1.0\n",
				"new_df = new_df.withColumn('bathrooms', when(col('bathrooms').isNull(), rounded_mean_bathrooms).otherwise(col('bathrooms')))\n",
				"#new_df.filter(col('bathrooms').isNull()).count()\n",
				"new_df = new_df.withColumn(\"bathrooms\", round(col(\"bathrooms\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"\n",
				"#16)bedrooms:\n",
				"\n",
				"# replace null value using mean value\n",
				"#mean_bedrooms = new_df.select(mean(col('bedrooms'))).collect()[0][0]\n",
				"#1.65\n",
				"rounded_mean_bedrooms = 1.0\n",
				"new_df = new_df.withColumn('bedrooms', when(col('bedrooms').isNull(), rounded_mean_bedrooms).otherwise(col('bedrooms')))\n",
				"new_df = new_df.withColumn(\"bedrooms\", round(col(\"bedrooms\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"#17)beds:\n",
				"\n",
				"# replace null value using mean value\n",
				"from pyspark.sql.functions import col, mean, round,lit\n",
				"\n",
				"#mean_beds = new_df.select(mean(col('beds'))).collect()[0][0]\n",
				"#---> 2.59516   =======>  3.0\n",
				"\n",
				"rounded_mean_beds = 3.0\n",
				"new_df = new_df.withColumn('beds', when(col('beds').isNull(), rounded_mean_beds).otherwise(col('beds')))\n",
				"\n",
				"# make it integer\n",
				"new_df = new_df.withColumn(\"beds\", round(col(\"beds\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"#19)price column:\n",
				"\n",
				"from pyspark.sql.functions import col, mean, when\n",
				"#mean_price = new_df.filter(col(\"price\") != 0).select(mean(col(\"price\"))).first()[0]\n",
				"#mean_price=int(mean_price)\n",
				"mean_price =542\n",
				"new_df = new_df.withColumn(\"price\", when((col(\"price\").isNull()) | (col(\"price\") == 0), mean_price).otherwise(col(\"price\")))\n",
				"\n",
				"#20) security_deposit:\n",
				"\n",
				"#replacing NA values with zero\n",
				"new_df = new_df.fillna({'security_deposit': 0})\n",
				"\n",
				"\n",
				"#22)extra_people: zero null\n",
				"\n",
				"from pyspark.sql.functions import when\n",
				"\n",
				"# Replace null values in the column 'extra_people' with 0\n",
				"new_df = new_df.withColumn(\"extra_people\", when(new_df[\"extra_people\"].isNull(), 0.0).otherwise(new_df[\"extra_people\"]))\n",
				"new_df = new_df.withColumn(\"extra_people\", round(col(\"extra_people\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"#28)review_scores_rating:\n",
				"\n",
				"new_df = new_df.withColumn(\"review_scores_rating\",\n",
				"when(col(\"review_scores_rating\").isNull(), None)\n",
				".otherwise(floor((col(\"review_scores_rating\") - 10) / 9).cast(IntegerType())))\n",
				"\n",
				"#absolute_mean = new_df.filter(col(\"review_scores_rating\").isNotNull()).select(mean(col(\"review_scores_rating\"))).collect()[0][0]\n",
				"#print(absolute_mean)\n",
				"\n",
				"rounded_abs_mean = 7\n",
				"new_df = new_df.withColumn(\"review_scores_rating\", when(col(\"review_scores_rating\").isNull(), rounded_abs_mean).otherwise(col(\"review_scores_rating\")))\n",
				"\n",
				"\n",
				"#29)\"instant_bookable\":\n",
				"\n",
				"new_df = new_df.withColumn('instant_bookable', when(col('instant_bookable') == 't', 'yes')\n",
				".when(col('instant_bookable') == 'f', 'no')\n",
				".when(col('instant_bookable').isNull() | (col('instant_bookable') == ''), 'yes')\n",
				".otherwise(col('instant_bookable')))\n",
				"\n",
				"\n",
				"#30)month:\n",
				"\n",
				"new_df = new_df.withColumn(\"month\",\n",
				"when(col(\"month\") == 1, \"january\")\n",
				".when(col(\"month\") == 2, \"february\")\n",
				".when(col(\"month\") == 3, \"march\")\n",
				".when(col(\"month\") == 4, \"april\")\n",
				".when(col(\"month\") == 5, \"may\")\n",
				".when(col(\"month\") == 6, \"june\")\n",
				".when(col(\"month\") == 7, \"july\")\n",
				".when(col(\"month\") == 8, \"august\")\n",
				".when(col(\"month\") == 9, \"september\")\n",
				".when(col(\"month\") == 10, \"october\")\n",
				".when(col(\"month\") == 11, \"november\")\n",
				".when(col(\"month\") == 12, \"december\")\n",
				".otherwise(col(\"month\")))\n",
				"\n",
				"\n",
				"\n",
				"#38)\"minimum_minimum_nights\":\n",
				"\n",
				"#mean_value = new_df.select(mean(col(\"minimum_minimum_nights\"))).collect()[0][0]\n",
				"mean_value= 5.0\n",
				"new_df = new_df.withColumn(\"minimum_minimum_nights\", when(col(\"minimum_minimum_nights\").isNull(), mean_value).otherwise(col(\"minimum_minimum_nights\")))\n",
				"# make it integer\n",
				"new_df = new_df.withColumn(\"minimum_minimum_nights\", round(col(\"minimum_minimum_nights\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"\n",
				"#39)\"maximum_maximum_nights\":\n",
				"\n",
				"#mean_value = new_df.filter(col(\"maximum_maximum_nights\") != 999999999.0).select(mean(col(\"maximum_maximum_nights\"))).collect()[0][0]\n",
				"mean_value=1104\n",
				"new_df = new_df.withColumn(\"maximum_maximum_nights\", when((col(\"maximum_maximum_nights\") == 999999999.0) | (col(\"maximum_maximum_nights\").isNull()), mean_value).otherwise(col(\"maximum_maximum_nights\")))\n",
				"# make it integer\n",
				"new_df = new_df.withColumn(\"maximum_maximum_nights\", round(col(\"maximum_maximum_nights\")).cast(\"int\"))\n",
				"\n",
				"\n",
				"\n",
				"# give new schema to df\n",
				"\n",
				"new_schema = StructType([\n",
				"StructField(\"id\", LongType()),\n",
				"StructField(\"name\", StringType()),\n",
				"StructField(\"host_name\", StringType()),\n",
				"StructField(\"host_response_time\", StringType()),\n",
				"StructField(\"host_listings_count\", IntegerType()),\n",
				"StructField(\"host_verifications\", StringType()),\n",
				"StructField(\"host_identity_verified\", StringType()),\n",
				"StructField(\"neighbourhood\", StringType()),\n",
				"StructField(\"city\", StringType()),\n",
				"StructField(\"latitude\", DoubleType()),\n",
				"StructField(\"longitude\", DoubleType()),\n",
				"StructField(\"property_type\", StringType()),\n",
				"StructField(\"room_type\", StringType()),\n",
				"StructField(\"accommodates\", IntegerType()),\n",
				"StructField(\"bathrooms\", IntegerType()),\n",
				"StructField(\"bedrooms\", IntegerType()),\n",
				"StructField(\"beds\", IntegerType()),\n",
				"StructField(\"amenities\", StringType()),\n",
				"StructField(\"guests_included\", IntegerType()),\n",
				"StructField(\"availability_30\", IntegerType()),\n",
				"StructField(\"availability_60\", IntegerType()),\n",
				"StructField(\"availability_90\", IntegerType()),\n",
				"StructField(\"availability_365\", IntegerType()),\n",
				"StructField(\"number_of_reviews\",IntegerType()),\n",
				"StructField(\"review_scores_rating\", IntegerType()),\n",
				"StructField(\"instant_bookable\", StringType()),\n",
				"StructField(\"month\", StringType()),\n",
				"StructField(\"minimum_minimum_nights\", IntegerType()),\n",
				"StructField(\"maximum_maximum_nights\", IntegerType()),\n",
				"StructField(\"calculated_host_listings_count\", IntegerType()),\n",
				"StructField(\"cancellation_policy\", StringType())\n",
				"])\n",
				"\n",
				"\n",
				"for field in new_schema:\n",
				"        new_df = new_df.withColumn(field.name, new_df[field.name].cast(field.dataType))\n",
				"\n",
				"\n",
				"        \n",
				"\n",
				"# to write in s3 bucket cleaned data in parquet format\n",
				"\n",
				"new_df.coalesce(1).write \\\n",
				"    .option(\"header\", \"True\") \\\n",
				"    .option(\"multiline\", True) \\\n",
				"    .mode(\"overwrite\") \\\n",
				"    .parquet(\"s3://airbnbcleanzone/airbnbclean/\")\n",
				"\n",
				"\n",
				"\n",
				"job.commit()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
