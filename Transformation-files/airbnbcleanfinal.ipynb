{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 000cd607-6b6f-4717-91c5-bf0703bb59bf.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 000cd607-6b6f-4717-91c5-bf0703bb59bf.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 000cd607-6b6f-4717-91c5-bf0703bb59bf.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 000cd607-6b6f-4717-91c5-bf0703bb59bf.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.transforms import ApplyMapping, DropNullFields\nfrom pyspark.sql.functions import lit, rand, col, regexp_replace, when, round\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, LongType\nfrom pyspark.sql.functions import col, floor\nfrom pyspark.sql.functions import col, mean, round,lit\n\n\n\n\n# rds data with col cancellation policy\n\ndf_rds_new = spark.read.format('parquet') \\\n    .option(\"mode\", \"PERMISSIVE\") \\\n    .option(\"header\", \"true\") \\\n    .load('s3://airbnbraw/rdsdata2/rdsdata2.parquet/part-00000-7209b082-404e-4dc4-bf42-a609b50cb674-c000.snappy.parquet')\n\n# old data\ndf = spark.read.format('csv').options(sep=\",\", escape='\"', mode=\"PERMISSIVE\", header=True, multiLine=True).load('s3://final-044/zip/total_data.csv')\n\n\nselected_columns = [\n\"id\", \"name\", \"host_name\", \"host_response_time\", \"host_listings_count\", \"host_verifications\",\n\"host_identity_verified\", \"neighbourhood\", \"city\", \"latitude\", \"longitude\", \"property_type\",\n\"room_type\", \"accommodates\", \"bathrooms\", \"bedrooms\", \"beds\", \"amenities\", \"price\",\n\"security_deposit\", \"guests_included\", \"extra_people\", \"availability_30\", \"availability_60\",\n\"availability_90\", \"availability_365\", \"number_of_reviews\", \"review_scores_rating\",\n\"instant_bookable\", \"month\", \"minimum_minimum_nights\", \"maximum_maximum_nights\",\n\"calculated_host_listings_count\",\"cancellation_policy\"\n]\n\n# Selecting columns\ndf = df.select(selected_columns)\ndf1_rds_new = df_rds_new.select(selected_columns)\n\n\n# Union dataframes\nnew_df = df.union(df1_rds_new)\n\n\n\n#handle duplicates\ndf8=new_df.dropDuplicates()\nnew_df=df8\n\n\n\n\n# replace $\ncolumns_to_replace = [\"price\", \"security_deposit\", \"extra_people\"]\nfor col_name in columns_to_replace:\n    new_df = new_df.withColumn(col_name, regexp_replace(col(col_name), \"\\\\$\", \"\"))\n\n# 2)name:\nnew_df = new_df.dropna(subset=[\"name\"])\n\n# 3)host_name:\nnew_df = new_df.dropna(subset=[\"host_name\"])\n\n\n#4)host_response_time:\n\n#mode_time = new_df.groupBy(\"host_response_time\").count().orderBy(col(\"count\").desc()).limit(1).select(\"host_response_time\").collect()[0][0]\nmode_time= \"within an hour\"\nnew_df = new_df.fillna({\"host_response_time\": \"within an hour\"})\n\n\n\n#5) host_listings_count:\nnew_df = new_df.withColumn(\"host_listings_count\", round(col(\"host_listings_count\")).cast(\"int\"))\n\n\n#8)neighbourhood:\n\n# Replace null values in the neighbourhood column with \"Unknown\"\nnew_df = new_df.withColumn(\"neighbourhood\", when(new_df[\"neighbourhood\"].isNull(), \"Unknown\").otherwise(new_df[\"neighbourhood\"]))\n\n\n#9)city:\n\ndefault_value = \"Rio\"\nnew_df = new_df.withColumn(\"city\", when(new_df[\"city\"].isNull(), default_value).otherwise(new_df[\"city\"]))\n\n\n#15)bathrooms:\n# replace null value using mean value\n\nfrom pyspark.sql.functions import col, mean, round,lit\n#mean_bathrooms = new_df.select(mean(col('bathrooms'))).collect()[0][0]\n#--> 1.6951  ====> 1.0\nrounded_mean_bathrooms = 1.0\nnew_df = new_df.withColumn('bathrooms', when(col('bathrooms').isNull(), rounded_mean_bathrooms).otherwise(col('bathrooms')))\n#new_df.filter(col('bathrooms').isNull()).count()\nnew_df = new_df.withColumn(\"bathrooms\", round(col(\"bathrooms\")).cast(\"int\"))\n\n\n\n#16)bedrooms:\n\n# replace null value using mean value\n#mean_bedrooms = new_df.select(mean(col('bedrooms'))).collect()[0][0]\n#1.65\nrounded_mean_bedrooms = 1.0\nnew_df = new_df.withColumn('bedrooms', when(col('bedrooms').isNull(), rounded_mean_bedrooms).otherwise(col('bedrooms')))\nnew_df = new_df.withColumn(\"bedrooms\", round(col(\"bedrooms\")).cast(\"int\"))\n\n\n#17)beds:\n\n# replace null value using mean value\nfrom pyspark.sql.functions import col, mean, round,lit\n\n#mean_beds = new_df.select(mean(col('beds'))).collect()[0][0]\n#---> 2.59516   =======>  3.0\n\nrounded_mean_beds = 3.0\nnew_df = new_df.withColumn('beds', when(col('beds').isNull(), rounded_mean_beds).otherwise(col('beds')))\n\n# make it integer\nnew_df = new_df.withColumn(\"beds\", round(col(\"beds\")).cast(\"int\"))\n\n\n#19)price column:\n\nfrom pyspark.sql.functions import col, mean, when\n#mean_price = new_df.filter(col(\"price\") != 0).select(mean(col(\"price\"))).first()[0]\n#mean_price=int(mean_price)\nmean_price =542\nnew_df = new_df.withColumn(\"price\", when((col(\"price\").isNull()) | (col(\"price\") == 0), mean_price).otherwise(col(\"price\")))\n\n#20) security_deposit:\n\n#replacing NA values with zero\nnew_df = new_df.na.fill({'security_deposit': 0})\n\n\n\n#22)extra_people: zero null\n\nfrom pyspark.sql.functions import when\n\n# Replace null values in the column 'extra_people' with 0\nnew_df = new_df.fillna({\"extra_people\": 0.0})\nnew_df = new_df.withColumn(\"extra_people\", round(col(\"extra_people\")).cast(\"int\"))\n\n\n\n#28)review_scores_rating:\n\nnew_df = new_df.withColumn(\"review_scores_rating\",\nwhen(col(\"review_scores_rating\").isNull(), None)\n.otherwise(floor((col(\"review_scores_rating\") - 10) / 9).cast(IntegerType())))\n\n#absolute_mean = new_df.filter(col(\"review_scores_rating\").isNotNull()).select(mean(col(\"review_scores_rating\"))).collect()[0][0]\n#print(absolute_mean)\n\nrounded_abs_mean = 7\nnew_df = new_df.withColumn(\"review_scores_rating\", when(col(\"review_scores_rating\").isNull(), rounded_abs_mean).otherwise(col(\"review_scores_rating\")))\n\n\n#29)\"instant_bookable\":\n\nnew_df = new_df.withColumn('instant_bookable', when(col('instant_bookable') == 't', 'yes')\n.when(col('instant_bookable') == 'f', 'no')\n.when(col('instant_bookable').isNull() | (col('instant_bookable') == ''), 'yes')\n.otherwise(col('instant_bookable')))\n\n\n#30)month:\n\nnew_df = new_df.withColumn(\"month\",\nwhen(col(\"month\") == 1, \"january\")\n.when(col(\"month\") == 2, \"february\")\n.when(col(\"month\") == 3, \"march\")\n.when(col(\"month\") == 4, \"april\")\n.when(col(\"month\") == 5, \"may\")\n.when(col(\"month\") == 6, \"june\")\n.when(col(\"month\") == 7, \"july\")\n.when(col(\"month\") == 8, \"august\")\n.when(col(\"month\") == 9, \"september\")\n.when(col(\"month\") == 10, \"october\")\n.when(col(\"month\") == 11, \"november\")\n.when(col(\"month\") == 12, \"december\")\n.otherwise(col(\"month\")))\n\n\n\n#38)\"minimum_minimum_nights\":\n\n#mean_value = new_df.select(mean(col(\"minimum_minimum_nights\"))).collect()[0][0]\nmean_value= 5.0\nnew_df = new_df.withColumn(\"minimum_minimum_nights\", when(col(\"minimum_minimum_nights\").isNull(), mean_value).otherwise(col(\"minimum_minimum_nights\")))\n# make it integer\nnew_df = new_df.withColumn(\"minimum_minimum_nights\", round(col(\"minimum_minimum_nights\")).cast(\"int\"))\n\n\n\n#39)\"maximum_maximum_nights\":\n\n#mean_value = new_df.filter(col(\"maximum_maximum_nights\") != 999999999.0).select(mean(col(\"maximum_maximum_nights\"))).collect()[0][0]\nmean_value=1104\nnew_df = new_df.withColumn(\"maximum_maximum_nights\", when((col(\"maximum_maximum_nights\") == 999999999.0) | (col(\"maximum_maximum_nights\").isNull()), mean_value).otherwise(col(\"maximum_maximum_nights\")))\n# make it integer\nnew_df = new_df.withColumn(\"maximum_maximum_nights\", round(col(\"maximum_maximum_nights\")).cast(\"int\"))\n\n\n\n# give new schema to df\n\nnew_schema = StructType([\nStructField(\"id\", LongType()),\nStructField(\"name\", StringType()),\nStructField(\"host_name\", StringType()),\nStructField(\"host_response_time\", StringType()),\nStructField(\"host_listings_count\", IntegerType()),\nStructField(\"host_verifications\", StringType()),\nStructField(\"host_identity_verified\", StringType()),\nStructField(\"neighbourhood\", StringType()),\nStructField(\"city\", StringType()),\nStructField(\"latitude\", DoubleType()),\nStructField(\"longitude\", DoubleType()),\nStructField(\"property_type\", StringType()),\nStructField(\"room_type\", StringType()),\nStructField(\"accommodates\", IntegerType()),\nStructField(\"bathrooms\", IntegerType()),\nStructField(\"bedrooms\", IntegerType()),\nStructField(\"beds\", IntegerType()),\nStructField(\"amenities\", StringType()),\nStructField(\"extra_people\", FloatType()),\nStructField(\"availability_30\", IntegerType()),\nStructField(\"availability_60\", IntegerType()),\nStructField(\"availability_90\", IntegerType()),\nStructField(\"availability_365\", IntegerType()),\nStructField(\"number_of_reviews\",IntegerType()),\nStructField(\"review_scores_rating\", IntegerType()),\nStructField(\"instant_bookable\", StringType()),\nStructField(\"month\", StringType()),\nStructField(\"minimum_minimum_nights\", IntegerType()),\nStructField(\"maximum_maximum_nights\", IntegerType()),\nStructField(\"calculated_host_listings_count\", IntegerType()),\nStructField(\"cancellation_policy\", StringType())\n])\n\n\nfor field in new_schema:\n        new_df = new_df.withColumn(field.name, new_df[field.name].cast(field.dataType))\n\n\n        \n\n# to write in s3 bucket cleaned data in parquet format\n\nnew_df.coalesce(1).write \\\n    .option(\"header\", \"True\") \\\n    .option(\"multiline\", True) \\\n    .mode(\"overwrite\") \\\n    .parquet(\"s3://airbnbclean/cleandata/\")\n\n\n\njob.commit()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "new_df.filter(col(\"price\").isNull()).count()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}