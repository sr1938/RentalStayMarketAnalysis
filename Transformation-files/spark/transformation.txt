

df = spark.read.format('csv').options(sep=",", escape='"', mode="PERMISSIVE", header=True, multiLine=True).load('s3://final-044/zip/total_data.csv')


#to see one column name:
>>> df.select("property_type").show()


#print schema with datatypes:
>>> df.printSchema()


#show all column names:
column_names = df.columns
>>> print(column_names)

selected_columns = [
    "id", "name", "description", "transit", 
    "host_name", "host_response_time", "host_is_superhost", "host_listings_count", "host_verifications", 
    "host_identity_verified", "neighbourhood", "smart_location", "zipcode", "latitude", "longitude", 
    "property_type", "room_type", "accommodates", "bathrooms", "bedrooms", "beds", "amenities", 
    "price", "security_deposit", "guests_included", "extra_people", "has_availability", 
    "availability_30", "availability_60", "availability_90", "availability_365", 
    "review_scores_rating", "review_scores_accuracy", "review_scores_cleanliness", 
    "review_scores_checkin", "review_scores_communication", "review_scores_location", 
    "review_scores_value", "instant_bookable", "cancellation_policy", "month", 
    "minimum_minimum_nights", "maximum_minimum_nights", "minimum_maximum_nights", 
    "maximum_maximum_nights", "minimum_nights_avg_ntm", "maximum_nights_avg_ntm", 
    "number_of_reviews_ltm", "calculated_host_listings_count_entire_homes", 
    "calculated_host_listings_count_private_rooms", "calculated_host_listings_count_shared_rooms"
]

new_df = df.select(selected_columns)
new_df.printSchema()


# Replace 't' with 'yes' and 'f' with 'no' in the 'has_availability' column
filled_df = df.withColumn('has_availability', when(df['has_availability'] == 't', 'yes').when(df['has_availability'] == 'f', 'no').otherwise(df['has_availability']))

filled_df = df.withColumn('has_availability', when(col('has_availability') == 't', 'yes')
                           .when(col('has_availability') == 'f', 'no')
                           .when(col('has_availability').isNull() | (col('has_availability') == ''), 'no')
                           .otherwise(col('has_availability')))


from pyspark.sql import functions as F

# Calculate the absolute mean value and round it off to the nearest integer
absolute_mean_rounded = int(round(df.select(F.mean('minimum_minimum_nights')).collect()[0][0]))

# Replace null values with the rounded absolute mean value
filled_df = df.fillna(absolute_mean_rounded, subset=['minimum_minimum_nights'])


